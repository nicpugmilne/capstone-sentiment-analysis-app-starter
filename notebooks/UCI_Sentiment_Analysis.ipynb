{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# UCI Sentiment Analysis - Custom Keras Model\n",
        "\n",
        "This notebook creates a custom sentiment analysis model using the UCI Labelled Sentences dataset. The model will be trained on data from Yelp, Amazon, and IMDB sources to create a personalized sentiment analysis tool."
      ],
      "metadata": {
        "id": "intro-cell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Download the dataset and unzip it in Google Colab"
      ],
      "metadata": {
        "id": "download-section"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-dataset"
      },
      "outputs": [],
      "source": [
        "# download dataset from the UCI website\n",
        "!curl -o uci-labelled-sentences.zip https://archive.ics.uci.edu/static/public/331/sentiment+labelled+sentences.zip\n",
        "\n",
        "# unzip dataset in Colab\n",
        "!unzip uci-labelled-sentences.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Import Keras and other libraries"
      ],
      "metadata": {
        "id": "import-section"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import-libraries"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM\n",
        "from keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Load the datasets"
      ],
      "metadata": {
        "id": "load-data-section"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "load-datasets"
      },
      "outputs": [],
      "source": [
        "df_list = []\n",
        "\n",
        "# Yelp\n",
        "df_yelp = pd.read_csv('sentiment labelled sentences/yelp_labelled.txt', names=['sentence', 'label'], sep='\\t')\n",
        "df_yelp['source'] = 'yelp'\n",
        "df_list.append(df_yelp)\n",
        "\n",
        "# Amazon\n",
        "df_amazon = pd.read_csv('sentiment labelled sentences/amazon_cells_labelled.txt', names=['sentence', 'label'], sep='\\t')\n",
        "df_amazon['source'] = 'amazon'\n",
        "df_list.append(df_amazon)\n",
        "\n",
        "# IMDB\n",
        "df_imdb = pd.read_csv('sentiment labelled sentences/imdb_labelled.txt', names=['sentence', 'label'], sep='\\t')\n",
        "df_imdb['source'] = 'imdb'\n",
        "df_list.append(df_imdb)\n",
        "\n",
        "# Concatenate the dataframes\n",
        "df = pd.concat(df_list)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset-info"
      },
      "outputs": [],
      "source": [
        "# Display dataset information\n",
        "print(f\"Total number of sentences: {len(df)}\")\n",
        "print(f\"Number of positive labels: {len(df[df['label'] == 1])}\")\n",
        "print(f\"Number of negative labels: {len(df[df['label'] == 0])}\")\n",
        "print(f\"Sources: {df['source'].unique()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Tokenize the sentences"
      ],
      "metadata": {
        "id": "tokenize-section"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tokenize-sentences"
      },
      "outputs": [],
      "source": [
        "max_features = 2000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "tokenizer.fit_on_texts(df['sentence'].values)\n",
        "X = tokenizer.texts_to_sequences(df['sentence'].values)\n",
        "X = pad_sequences(X)\n",
        "y = df['label'].values\n",
        "\n",
        "print(f\"X shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Split the dataset"
      ],
      "metadata": {
        "id": "split-section"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "split-dataset"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.12)\n",
        "\n",
        "print(f\"Training set size: {len(X_train)}\")\n",
        "print(f\"Test set size: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Define the model"
      ],
      "metadata": {
        "id": "model-section"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "create-model"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(max_features, 64, input_length=X.shape[1]))\n",
        "  model.add(LSTM(16))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "model = create_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Train the model"
      ],
      "metadata": {
        "id": "train-section"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train-model"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train, y_train, \n",
        "                    epochs=6, \n",
        "                    batch_size=16, \n",
        "                    validation_data=(X_test, y_test), \n",
        "                    callbacks=[EarlyStopping(monitor='val_accuracy', \n",
        "                                            min_delta=0.001, \n",
        "                                            patience=2, \n",
        "                                            verbose=1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate-model"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test loss: {test_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Test the model with sample sentences"
      ],
      "metadata": {
        "id": "test-section"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test-samples"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment(text):\n",
        "    # Preprocess the text\n",
        "    sequences = tokenizer.texts_to_sequences([text])\n",
        "    padded = pad_sequences(sequences, maxlen=X.shape[1])\n",
        "    \n",
        "    # Make prediction\n",
        "    prediction = model.predict(padded)\n",
        "    return prediction[0][0]\n",
        "\n",
        "# Test with sample sentences\n",
        "test_sentences = [\n",
        "    \"This product is amazing! I love it!\",\n",
        "    \"This is the worst thing I've ever bought.\",\n",
        "    \"The food was okay, nothing special.\",\n",
        "    \"Excellent service and great quality!\",\n",
        "    \"Terrible experience, would not recommend.\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    score = predict_sentiment(sentence)\n",
        "    sentiment = \"Positive\" if score > 0.5 else \"Negative\"\n",
        "    print(f\"'{sentence}' -> Score: {score:.4f} ({sentiment})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Save the model and the tokenizer"
      ],
      "metadata": {
        "id": "save-section"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save-model"
      },
      "outputs": [],
      "source": [
        "# Save the trained model\n",
        "model.save(\"uci_sentimentanalysis.h5\")\n",
        "\n",
        "# Save the tokenizer\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.DEFAULT_PROTOCOL)\n",
        "\n",
        "print(\"Model and tokenizer saved successfully!\")\n",
        "print(\"Files created:\")\n",
        "print(\"- uci_sentimentanalysis.h5\")\n",
        "print(\"- tokenizer.pickle\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Download the files\n",
        "\n",
        "After running all cells above, download the model and tokenizer files to your computer. These files will be used in the Flask application."
      ],
      "metadata": {
        "id": "download-files"
      }
    }
  ]
}
